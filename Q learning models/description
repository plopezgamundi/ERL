These are the Matlab scripts for the Q-learning algorithms. 

We quantified the reward choice data using a Rescorla-Wagner learning rule (Rescorla and Wagner, 1972; Watkins and Dayan, 1992; Sutton and Barto, 1998). Specifically, the stimulus value W(t) for the selected choice were updated as follows:
 W(t+1)=W(t)+ α ∙(R(t)-W(t)), where α indicates the learning rates (constrained between 0 and 1) and R(t) indicates the reward amount (1: reward, -1: no reward)  at trial t. In our paradigm, reward feedback was given on every trial, but rewards were only delivered on successful effort trials. Thus, we expected that reward information after a positive performance feedback would be more salient than reward information received after negative performance feedback, since these rewards would be hypothetical. To better capture individual differences in weighing of reward feedback received under the two feedback conditions, we used a dual learning rate reinforcement model (Collins and Frank, 2014; Garrett and Daw, 2020) with two learning rates: α^+ and α^-. This model allows updates to occur differently based on the performance outcome on the effort task. Specifically, updates to the stimulus value W(t) apply α^+ if positive performance feedback was received on trial t and apply α^- if a given trial was not successfully completed. Models were fit independently to the data for high and low effort blocks such that there were two seperate learning rates for high (α_H^+, α_H^-) and low (α_L^+, α_L^-) effort. Softmax action selection was used to compute the probability of choosing one of the following two stimuli (A or B):
P_A (t)=  (e^(β∙W) A^((t)))/(e^(β∙W) A^((t))+ e^(β∙W) B^((t)) ), where β is the inverse temperature parameter. β was constrained from 0 to 100 and determines the degree to which choices are made in a more deterministic or stochastic manner. The model was run 10 times, using random initial values for each subject by maximizing the log likelihood estimate with the fmincon function of MATLAB R2021. The parameters α and β with the best log likelihood estimate were selected. 
